{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417a6abe-66c1-4e1b-81af-d56b9954dde4",
   "metadata": {},
   "source": [
    "# Wikipedia Access & Album Parsing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7903cc-8c73-41b1-89d1-d205a17f42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import datetime as dt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580e1d61-5dc6-4018-962f-5a61d471fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaAlbumInfo:\n",
    "    WP_ALBUM_YEAR_INDEX_URL = 'https://en.wikipedia.org/wiki/Category:Lists_of_albums_by_release_date'\n",
    "    \n",
    "    def __init__(self, url_cache):\n",
    "        self.url_cache = url_cache\n",
    "\n",
    "    def _get_year_urls_raw(self):\n",
    "        r = self.url_cache.get(self.WP_ALBUM_YEAR_INDEX_URL)\n",
    "        return r['content']\n",
    "\n",
    "    def _parse_year_list_urls(self, html_text):\n",
    "        html_doc = BeautifulSoup(html_text)\n",
    "        results = []\n",
    "        for a_tag in html_doc.find_all('a'):\n",
    "            if 'href' in a_tag.attrs and 'title' in a_tag.attrs is not None:\n",
    "                href = a_tag.attrs['href']\n",
    "                title = a_tag.attrs['title']\n",
    "                m = re.search(r'List of (\\d+) albums', title)\n",
    "                if m:\n",
    "                    year = int(m.group(1))\n",
    "                    results.append({\n",
    "                        'year': year,\n",
    "                        'title': title,\n",
    "                        'url': urljoin(self.WP_ALBUM_YEAR_INDEX_URL, href)\n",
    "                    })\n",
    "        results = pd.DataFrame(results).sort_values(by='year', ascending=False)\n",
    "        return results\n",
    "    \n",
    "    def get_year_urls(self):\n",
    "        html_text = self._get_year_urls_raw()\n",
    "        return self._parse_year_list_urls(html_text)\n",
    "    \n",
    "class TableExtractor:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dataframes(h):\n",
    "        if not isinstance(h, BeautifulSoup):\n",
    "            h = BeautifulSoup(h)\n",
    "            \n",
    "        dfs = []\n",
    "        for tag_table in h.find_all('table'):\n",
    "            try:\n",
    "                dfs.append(TableExtractor.to_dataframe(tag_table))\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not extract table - {e}\")\n",
    "                #raise(e)\n",
    "        return dfs\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_dataframe(tag_table):\n",
    "        rows = TableExtractor.extract_rows(tag_table)\n",
    "        return pd.DataFrame(TableExtractor.to_table_dict(rows))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_cell(tag):\n",
    "        cell = {\n",
    "            'type': tag.name,\n",
    "            'text': tag.text\n",
    "        }\n",
    "        if 'rowspan' in tag.attrs:\n",
    "            cell['rowspan'] = int(tag.attrs['rowspan'])\n",
    "        if 'colspan' in tag.attrs:\n",
    "            cell['colspan'] = int(tag.attrs['colspan'])\n",
    "\n",
    "        links = []\n",
    "        for tag_a in tag.find_all('a'):\n",
    "            if \"href\" in tag_a.attrs:\n",
    "                links.append({ 'href': tag_a.attrs['href'], 'text': tag_a.text})\n",
    "        cell['Links'] = links\n",
    "        return cell\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_row(tag_tr):\n",
    "        row = []\n",
    "        for tag in tag_tr.children:\n",
    "            if tag.name in {'td', 'th'}:\n",
    "                row.append(TableExtractor.extract_cell(tag))\n",
    "        return row\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_rows(tag_table):\n",
    "        rows = []\n",
    "        \n",
    "        rows = [TableExtractor.extract_row(tag_tr) for tag_tr in tag_table.find_all('tr')]\n",
    "        rows = TableExtractor.apply_expansions(rows)\n",
    "        return rows\n",
    "        \n",
    "    @staticmethod\n",
    "    def apply_expansions(rows):\n",
    "        rows = TableExtractor.apply_colspan(rows)\n",
    "        rows = TableExtractor.apply_rowspan(rows)\n",
    "        return rows\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_rowspan(rows):\n",
    "        n_rows = [row.copy() for row in rows]\n",
    "        \n",
    "        for row_no in range(0, len(n_rows)):\n",
    "            cur_row = n_rows[row_no]\n",
    "            for col_no in range(0, len(cur_row)):\n",
    "                cell = cur_row[col_no]\n",
    "                if 'rowspan' in cell:\n",
    "                    #print(f\"Applying rowspan: {cell}\")\n",
    "                    rowspan = cell['rowspan']\n",
    "                    cell = cell.copy()\n",
    "                    del(cell['rowspan'])\n",
    "                    n_rows[row_no][col_no] = cell\n",
    "                    for row_delta in range(1, rowspan):\n",
    "                        if (row_no + row_delta) < len(n_rows):\n",
    "                            n_rows[row_no + row_delta].insert(col_no, cell.copy())\n",
    "        \n",
    "        return n_rows\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_colspan(rows):\n",
    "        n_rows = []\n",
    "        for row in rows:\n",
    "            n_row = []\n",
    "            for cell in row:\n",
    "                if 'colspan' in cell:\n",
    "                    cell = cell.copy()\n",
    "                    colspan = cell['colspan']\n",
    "                    del(cell['colspan'])\n",
    "                    for i in range(0, colspan):\n",
    "                        n_row.append(cell.copy())\n",
    "                else:\n",
    "                    n_row.append(cell)\n",
    "            n_rows.append(n_row)\n",
    "        return n_rows\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_header_row_no(rows):\n",
    "        for row_no in range(0, len(rows)):\n",
    "            row = rows[row_no]\n",
    "            if len(row) > 0 and row[0]['type'] == 'th':\n",
    "                return row_no\n",
    "            \n",
    "    @staticmethod\n",
    "    def to_table_dict(rows):\n",
    "        hrow_no = TableExtractor.find_header_row_no(rows)\n",
    "        row_start = 0\n",
    "        if hrow_no is not None:\n",
    "            row_start = hrow_no + 1\n",
    "            col_names = [cell['text'].strip() for cell in rows[hrow_no]]\n",
    "        else:\n",
    "            col_names = [f\"Unnamed:{i}\" for i in range(0, len(rows[0]))]\n",
    "        \n",
    "        data_rows = []\n",
    "        for row_no in range(row_start, len(rows)):\n",
    "            row = rows[row_no]\n",
    "            data_row = {}\n",
    "            links = {}\n",
    "            for h_i, h_n in enumerate(col_names):\n",
    "                if h_i < len(row):\n",
    "                    data_row[h_n] = row[h_i]['text'].strip()\n",
    "                    if 'Links' in row[h_i] and len(row[h_i]['Links']) > 0:\n",
    "                        links[h_n] = row[h_i]['Links']\n",
    "            data_row['Links'] = links\n",
    "            data_rows.append(data_row)\n",
    "\n",
    "        return data_rows\n",
    "\n",
    "class AlbumListExtractor:\n",
    "    \n",
    "    HEADER_RE = re.compile(r'h[1-7]')\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_album_dataframe(h):\n",
    "        albums = []\n",
    "        for ls in AlbumListExtractor.get_album_lists(h):\n",
    "            for item in ls['items']:\n",
    "                a = AlbumListExtractor.parse_list_album_item(item)\n",
    "                if a is not None:\n",
    "                    albums.append(a)\n",
    "        return pd.DataFrame(albums)\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_list_album_item(item):\n",
    "        parts = re.split(r'[\\u2013\\-]', item['text'], 2)\n",
    "        if len(parts) < 2:\n",
    "            print(f\"[WARN] - {item}\")\n",
    "            return None\n",
    "\n",
    "        album = {}\n",
    "        album['Album'] = parts[0].strip()\n",
    "        album['Artist'] = parts[1].strip()\n",
    "\n",
    "        album_link = None\n",
    "        artist_link = None\n",
    "\n",
    "        if 'Links' in item:\n",
    "            for link in item['Links']:\n",
    "                t = link['text'].strip()\n",
    "                href = link['href']\n",
    "                if t == album['Album']:\n",
    "                    if album_link is None or \"album\" in href.lower():\n",
    "                        album_link = link\n",
    "                if t == album['Artist']:\n",
    "                    if not(\"album\" in href.lower()):\n",
    "                        artist_link = link\n",
    "\n",
    "        album['Links'] = {}\n",
    "        if album_link is not None:\n",
    "            album['Links']['Album'] = [album_link]\n",
    "        if artist_link is not None:\n",
    "            album['Links']['Artist'] = [artist_link]\n",
    "\n",
    "        return album   \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_album_lists(h):\n",
    "        ls = AlbumListExtractor.get_all_lists(h)\n",
    "        return [l for l in ls if \"album\" in l['title'].lower()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_all_lists(h):\n",
    "        if not isinstance(h, BeautifulSoup):\n",
    "            h = BeautifulSoup(h)\n",
    "        \n",
    "        lists = []\n",
    "        for ul_tag in h.find_all('ul'):\n",
    "            lists.append(AlbumListExtractor.get_list(ul_tag))\n",
    "        return lists\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_list(list_tag):\n",
    "        prev_header = list_tag.find_previous(AlbumListExtractor.HEADER_RE)\n",
    "        if prev_header is not None:\n",
    "            list_title = prev_header.text\n",
    "        else:\n",
    "            list_title = None\n",
    "            \n",
    "        items = []\n",
    "        for li_tag in list_tag.find_all('li'):\n",
    "            item = {\n",
    "                'text': li_tag.text\n",
    "            }\n",
    "            links = []\n",
    "            for a_tag in li_tag.find_all('a'):\n",
    "                if 'href' in a_tag.attrs:\n",
    "                    links.append({ 'href': a_tag.attrs['href'], 'text': a_tag.text })\n",
    "            if len(links) > 0:\n",
    "                item['Links'] = links\n",
    "\n",
    "            items.append(item)\n",
    "\n",
    "        return { 'title': list_title, 'items': items }\n",
    "            \n",
    "    \n",
    "class WikiListPageAlbumExtractor:\n",
    "    \n",
    "    def __init__(self, url_cache):\n",
    "        self.url_cache = url_cache\n",
    "        \n",
    "    def get_albums(self, url):\n",
    "        r = self.url_cache.get(url)\n",
    "        h = r['content']\n",
    "        h_doc = BeautifulSoup(h)\n",
    "        \n",
    "        dfs = TableExtractor.get_dataframes(h_doc)\n",
    "        dfs = [df for df in dfs if \"Album\" in df.columns]\n",
    "    \n",
    "        # If no albums were found - fall back to list extractor\n",
    "        if len(dfs) == 0:\n",
    "            df = AlbumListExtractor.get_album_dataframe(h_doc)\n",
    "            if df is not None:\n",
    "                dfs = [df]\n",
    "                \n",
    "        for df in dfs:\n",
    "            df['Source'] = url\n",
    "                \n",
    "        return dfs\n",
    "\n",
    "    def get_all_dataframes(self, year_lists):\n",
    "        all_album_dfs = []\n",
    "        for _, row in year_lists.iterrows():\n",
    "            for album_df in self.get_albums(row.url):\n",
    "                album_df['Year'] = row['year']\n",
    "                all_album_dfs.append(album_df)\n",
    "        return all_album_dfs\n",
    "    \n",
    "    def get_all_albums(self, year_lists):\n",
    "        all_dfs = self.get_all_dataframes(year_lists)\n",
    "        album_df = (pd.concat(\n",
    "            [df for df in all_dfs if \"Album\" in df.columns],\n",
    "            ignore_index=True\n",
    "        )[['Artist', 'Album', 'Genre', 'Label', 'Year', 'Links', 'Source']])\n",
    "        \n",
    "        for _, row in album_df.iterrows():\n",
    "            self.clean_links(row['Source'] ,row['Links'])\n",
    "        \n",
    "        return album_df\n",
    "\n",
    "    def clean_links(self, source_url, links):\n",
    "        for cat, cat_links in links.items():\n",
    "            for link in cat_links:\n",
    "                link['href'] = urljoin(source_url, link['href'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4731994-d486-440d-b13a-4f9bddd71982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
