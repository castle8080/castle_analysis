{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "417a6abe-66c1-4e1b-81af-d56b9954dde4",
   "metadata": {},
   "source": [
    "# Wikipedia Access & Album Parsing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7903cc-8c73-41b1-89d1-d205a17f42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import datetime as dt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580e1d61-5dc6-4018-962f-5a61d471fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaAlbumInfo:\n",
    "    WP_ALBUM_YEAR_INDEX_URL = 'https://en.wikipedia.org/wiki/Category:Lists_of_albums_by_release_date'\n",
    "    \n",
    "    def __init__(self, url_cache):\n",
    "        self.url_cache = url_cache\n",
    "\n",
    "    def _get_year_urls_raw(self):\n",
    "        r = self.url_cache.get(self.WP_ALBUM_YEAR_INDEX_URL)\n",
    "        return r['content']\n",
    "\n",
    "    def _parse_year_list_urls(self, html_text):\n",
    "        html_doc = BeautifulSoup(html_text)\n",
    "        results = []\n",
    "        for a_tag in html_doc.find_all('a'):\n",
    "            if 'href' in a_tag.attrs and 'title' in a_tag.attrs is not None:\n",
    "                href = a_tag.attrs['href']\n",
    "                title = a_tag.attrs['title']\n",
    "                m = re.search(r'List of (\\d+) albums', title)\n",
    "                if m:\n",
    "                    year = int(m.group(1))\n",
    "                    results.append({\n",
    "                        'year': year,\n",
    "                        'title': title,\n",
    "                        'url': urljoin(self.WP_ALBUM_YEAR_INDEX_URL, href)\n",
    "                    })\n",
    "        results = pd.DataFrame(results).sort_values(by='year', ascending=False)\n",
    "        return results\n",
    "    \n",
    "    def get_year_urls(self):\n",
    "        html_text = self._get_year_urls_raw()\n",
    "        return self._parse_year_list_urls(html_text)\n",
    "    \n",
    "class TableExtractor:\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_dataframes(h):\n",
    "        if not isinstance(h, BeautifulSoup):\n",
    "            h = BeautifulSoup(h)\n",
    "            \n",
    "        dfs = []\n",
    "        for tag_table in h.find_all('table'):\n",
    "            try:\n",
    "                dfs.append(TableExtractor.to_dataframe(tag_table))\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Could not extract table - {e}\")\n",
    "                #raise(e)\n",
    "        return dfs\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_dataframe(tag_table):\n",
    "        rows = TableExtractor.extract_rows(tag_table)\n",
    "        return pd.DataFrame(TableExtractor.to_table_dict(rows))\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_cell(tag):\n",
    "        cell = {\n",
    "            'type': tag.name,\n",
    "            'text': tag.text\n",
    "        }\n",
    "        if 'rowspan' in tag.attrs:\n",
    "            cell['rowspan'] = int(tag.attrs['rowspan'])\n",
    "        if 'colspan' in tag.attrs:\n",
    "            cell['colspan'] = int(tag.attrs['colspan'])\n",
    "\n",
    "        links = []\n",
    "        for tag_a in tag.find_all('a'):\n",
    "            if \"href\" in tag_a.attrs:\n",
    "                links.append({ 'href': tag_a.attrs['href'], 'text': tag_a.text})\n",
    "        cell['links'] = links\n",
    "        return cell\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_row(tag_tr):\n",
    "        row = []\n",
    "        for tag in tag_tr.children:\n",
    "            if tag.name in {'td', 'th'}:\n",
    "                row.append(TableExtractor.extract_cell(tag))\n",
    "        return row\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_rows(tag_table):\n",
    "        rows = []\n",
    "        \n",
    "        rows = [TableExtractor.extract_row(tag_tr) for tag_tr in tag_table.find_all('tr')]\n",
    "        rows = TableExtractor.apply_expansions(rows)\n",
    "        return rows\n",
    "        \n",
    "    @staticmethod\n",
    "    def apply_expansions(rows):\n",
    "        rows = TableExtractor.apply_colspan(rows)\n",
    "        rows = TableExtractor.apply_rowspan(rows)\n",
    "        return rows\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_rowspan(rows):\n",
    "        n_rows = [row.copy() for row in rows]\n",
    "        \n",
    "        for row_no in range(0, len(n_rows)):\n",
    "            cur_row = n_rows[row_no]\n",
    "            for col_no in range(0, len(cur_row)):\n",
    "                cell = cur_row[col_no]\n",
    "                if 'rowspan' in cell:\n",
    "                    #print(f\"Applying rowspan: {cell}\")\n",
    "                    rowspan = cell['rowspan']\n",
    "                    cell = cell.copy()\n",
    "                    del(cell['rowspan'])\n",
    "                    n_rows[row_no][col_no] = cell\n",
    "                    for row_delta in range(1, rowspan):\n",
    "                        if (row_no + row_delta) < len(n_rows):\n",
    "                            n_rows[row_no + row_delta].insert(col_no, cell.copy())\n",
    "        \n",
    "        return n_rows\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_colspan(rows):\n",
    "        n_rows = []\n",
    "        for row in rows:\n",
    "            n_row = []\n",
    "            for cell in row:\n",
    "                if 'colspan' in cell:\n",
    "                    cell = cell.copy()\n",
    "                    colspan = cell['colspan']\n",
    "                    del(cell['colspan'])\n",
    "                    for i in range(0, colspan):\n",
    "                        n_row.append(cell.copy())\n",
    "                else:\n",
    "                    n_row.append(cell)\n",
    "            n_rows.append(n_row)\n",
    "        return n_rows\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_header_row_no(rows):\n",
    "        for row_no in range(0, len(rows)):\n",
    "            row = rows[row_no]\n",
    "            if len(row) > 0 and row[0]['type'] == 'th':\n",
    "                return row_no\n",
    "            \n",
    "    @staticmethod\n",
    "    def to_table_dict(rows):\n",
    "        hrow_no = TableExtractor.find_header_row_no(rows)\n",
    "        row_start = 0\n",
    "        if hrow_no is not None:\n",
    "            row_start = hrow_no + 1\n",
    "            col_names = [cell['text'].strip() for cell in rows[hrow_no]]\n",
    "        else:\n",
    "            col_names = [f\"Unnamed:{i}\" for i in range(0, len(rows[0]))]\n",
    "        \n",
    "        data_rows = []\n",
    "        for row_no in range(row_start, len(rows)):\n",
    "            row = rows[row_no]\n",
    "            data_row = {}\n",
    "            links = {}\n",
    "            for h_i, h_n in enumerate(col_names):\n",
    "                if h_i < len(row):\n",
    "                    data_row[h_n] = row[h_i]['text'].strip()\n",
    "                    if 'links' in row[h_i] and len(row[h_i]['links']) > 0:\n",
    "                        links[h_n] = row[h_i]['links']\n",
    "            data_row['links'] = links\n",
    "            data_rows.append(data_row)\n",
    "\n",
    "        return data_rows\n",
    "\n",
    "class WikiListPageAlbumExtractor:\n",
    "    \n",
    "    def __init__(self, url_cache):\n",
    "        self.url_cache = url_cache\n",
    "        \n",
    "    def get_albums(self, url):\n",
    "        r = self.url_cache.get(url)\n",
    "        h = r['content']\n",
    "        h_doc = BeautifulSoup(h)\n",
    "        \n",
    "        return TableExtractor.get_dataframes(h_doc)\n",
    "    \n",
    "    def get_all_dataframes(self, year_lists):\n",
    "        all_album_dfs = []\n",
    "        for _, row in year_lists.iterrows():\n",
    "            for album_df in self.get_albums(row.url):\n",
    "                album_df['year'] = row['year']\n",
    "                all_album_dfs.append(album_df)\n",
    "        return all_album_dfs\n",
    "    \n",
    "    def get_all_albums(self, year_lists):\n",
    "        all_dfs = self.get_all_dataframes(year_lists)\n",
    "        return (pd.concat(\n",
    "            [df for df in all_dfs if \"Album\" in df.columns],\n",
    "            ignore_index=True\n",
    "        )[['Artist', 'Album', 'Genre', 'Label', 'year', 'links']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4731994-d486-440d-b13a-4f9bddd71982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
